### 对抗性攻击方法与算法

机器学习模型的鲁棒性在很大程度上取决于攻击者找到尽可能接近原始输入的对抗样本的能力

###### L-BFGS:L-BFGS（limitted Broyden-FletcherGoldfarb-Shanno）方法，该方法使用昂贵的线性搜索方法来寻找对抗样本的最优值。

######  Fast Gradient Sign Method (FGSM):通过寻找损失中正变化的最大方向来创建敌对示例。与L-BFGS方法相比，这是一种更快的方法，因为在每个级别上，仅沿符号梯度的方向执行一步梯度更新。

###### Basic Iterative Method (BIM):快速梯度符号法和类似的攻击方法的一个主要局限性是，它们的工作原理是假设对手样本可以直接输入机器学习模型。这远远不实际，因为大多数攻击者会试图通过传感器等设备访问机器学习模型[57]。文[58]中提出的基本迭代方法通过多次迭代运行梯度更新来克服这一限制。

###### Jacobian-based Saliency Map Attack (JSMA)：对于攻击，计算给定样本的雅可比矩阵，以找到对输出影响最大的样本的输入特征。随后，基于该输入特征创建小扰动以生成对抗攻击。

###### DeepFool:  作为一种通过找出原始输入和判定边界之间的最近距离来创建对抗性示例的方法。他们能够确定，通过使用相关分类器，与创建对抗性样本的最小扰动相对应的最近距离将是到相关分类器超平面的距离。

###### Carlini and Wagner Attack:专门针对现有的对抗性防御方法开发了目标攻击。研究发现，防御蒸馏等防御措施对卡里尼和瓦格纳的进攻无效。 

### 对抗攻击分类

#### 基于ML的端点保护对抗性攻击

网络安全中端点保护的一个主要组成部分是**恶意软件检测**。仅依赖数字签名的传统恶意软件检测系统变得不那么有效。在使用机器学习防止恶意软件攻击方面已经做出了重大努力。一些研究表明，这些机器学习模型容易受到攻击。

###### Iagodroid：最早针对基于机器学习的恶意软件检测系统的攻击之一是Iagodroid攻击[61]。Iagodroid使用一种方法，在恶意软件样本的筛选过程中诱导错误标记恶意软件家族。

###### Texture Perturbation Attacks（纹理扰动攻击）：研究人员部署了类似于计算机视觉的可视化技术，并将其用于恶意软件分类[62]。这涉及到将恶意软件二进制代码转换为图像数据。对抗性纹理恶意软件扰动攻击（ATMPA）在击败基于可视化的机器学习恶意软件检测系统方面取得了100%的效果，同时也导致了88.7%的传输率[19]。ATMPA的攻击模型允许攻击者在可视化过程中扭曲恶意软件图像数据。

###### EvnAttack:EvnAttack是在[32]中提出的一种规避攻击模型，它以双向方式操作恶意软件可执行文件的最佳特性部分，使得恶意软件能够基于观察到API调用对分类的贡献不同而逃避来自机器学习模型的检测恶意软件和良性文件。

###### AdvAttack:AdvAttack是在文献[30]中提出的一种以尽可能低的对抗代价逃避检测的新型攻击方法。这是通过操作API调用来实现的，方法是注入更多与良性文件最相关的功能，并删除那些与恶意软件关联度较高的功能。

###### MalGAN：为了克服传统的基于梯度的对抗实例生成算法的局限性，提出了一种基于生成性对抗网络（GAN）的对抗实例生成算法。生成模型主要用于通过将原始图像编码为低维潜在表示的输入重建[2]。原始输入的潜在表示可用于扭曲初始输入以创建对抗性样本。文献[63]提出的MalGAN利用生成建模技术，以接近于零的检测率避开黑箱恶意软件检测系统。

###### Slack Attacks（松弛攻击）：在[64]中引入了基于字节的卷积神经网络（MalConv）。与图像扰动攻击[29]不同，图像的保真度几乎不受关注，更改恶意软件文件二进制文件的攻击必须保持原始文件的语义保真度，因为任意更改恶意软件的字节可能会影响恶意软件的恶意效果。这个问题可以通过在二进制文件的末尾添加对抗性噪声来解决[33]。这可以防止添加的噪音影响恶意软件的功能。随机附加攻击和梯度附加攻击是两种类型的附加攻击，其工作原理是从均匀分布的样本中附加字节值，然后使用输入的梯度值逐步修改附加的字节值。文献[65]引入了两种附加攻击：良性附加攻击和FGM附加攻击，改进了以往攻击的长收敛时间。当恶意软件二进制文件超过模型的最大大小时，不可能向它们追加额外的字节。因此，[65]提出的松弛攻击利用恶意软件二进制文件的现有字节。slack攻击最常见的形式是slack  FGM攻击，它定义了一组可以自由修改的slack字节，而不会破坏恶意软件的功能。

#### 基于ML的网络保护对抗攻击

###### IDSGAN:IDSGAN是在[66]中提出的，用于生成针对入侵检测系统的对抗性攻击。IDSGAN基于Wasserstein  GAN[67]，它使用一个生成器、鉴别器和一个黑盒。该判别器用于模拟黑盒入侵检测系统，同时提供恶意流量样本。

###### TCP Obfuscation Techniques(TCP混淆技术)：另一种规避基于机器学习的入侵检测系统的方法是使用混淆技术。[68]提出了修改网络连接的各种属性，以混淆成功规避各种入侵检测分类器的TCP通信。

#### 基于应用安全的ML攻击

对统计垃圾邮件过滤器的攻击：一些垃圾邮件过滤器，如SpamAssasin、Spam  Bayes、Bogofilter，是基于流行的朴素贝叶斯机器学习算法，该算法于1998年首次应用于垃圾邮件过滤[69]。[70]引入的各种好词攻击成功地避免了机器学习模型检测垃圾邮件或垃圾邮件。

#### 针对用户行为分析的ML攻击

###### 针对人群控制检测系统的攻击：机器学习技术用于识别包括社交网络中的假用户在内的不当行为，并检测为网站支付假账户费用的用户。恶意的众包或众包系统用于将愿意付费的用户与进行恶意活动（如虚假新闻的生成和分发或恶意政治活动）的员工连接起来。机器学习模型已经被用来检测拥挤草坪的活动，准确率高达95%，特别是在检测拥挤草坪工人的帐户方面[71]。然而，恶意众包检测系统极易受到对手的规避和中毒攻击。

###### 针对ML的击键动力学攻击：作者[72]创建了敌对的击键样本，误导了原本准确的分类器接受人工生成的击键样本，将其视为属于真实用户。

#### 过程行为分析中ML的对抗性攻击

针对用于信用卡欺诈检测的ML的攻击：[73]研究了作为欺诈检测机制的logistic回归分类器如何受到攻击，从而导致一些欺诈交易未被检测到。以往的研究都是基于博弈论的相似模型来研究针对信用卡欺诈检测和电子邮件垃圾邮件检测的对抗性攻击。然而，作者引入了一个新的框架，与信用卡公司以前使用的模型相比，该框架成功地在验证集的多次迭代中产生了改进的AUC分数。

### 评估对抗风险

在讨论对抗性风险时，我们引入了机器学习模型的区分性和定向自主性的概念。**对抗性风险模型的双重目标是评估针对机器学习模型的对抗性攻击成功的可能性，以及攻击成功后的后果。**在本文中，我们提出了对抗风险模型，该模型基于机器学习模型在学习风格和任务方面的自主性水平。机器学习模型的区分自主和定向自主的概念代表了一种评估机器学习模型相对对抗风险的新方法。

###### Discriminative Autonomy（判别自治）：判别自治与机器学习模型执行的任务类型直接相关。分类等机器学习任务高度依赖于输入数据。因此，与生成性建模（generative  modeling）等在预测结果时较少依赖输入数据的任务相比，它们具有较低的辨别或条件自主性。

###### Directive autonomy（指令自主权）：机器学习模型的指令自主权是机器学习风格的函数。在有监督的机器学习中，由于模型需要先用某种形式的标记数据来学习，因此指令自主权较少。像强化学习这样的机器学习方式对使用任何形式的训练数据学习的模型依赖性较小，并且具有更高的指导自主性。

### 对抗性攻击防卫

巴雷诺等人。[15]  首先提出了三种防御机器学习算法对抗对抗性攻击的方法。**正则化、随机化和信息隐藏**。袁等人。[57]将防御分为两大策略。**主动策略和反应策略**。在本节中，我们将提供目前使用的最常见的攻击方法，并根据策略和方法对它们进行分类。

###### Gradient masking（梯度掩蔽）：梯度掩蔽方法修改机器学习模型，试图向攻击者掩盖其梯度。Nayebi等人[74]通过饱和乙状结肠网络证明了梯度掩蔽的效果，从而导致梯度消失效应。

###### Defensive Distillation（防御蒸馏）：蒸馏技术最初由Hinton等人提出。[78]用于将知识从大型神经网络转移到小型神经网络。它是由Papernot等人改编的。[75]通过使用原始神经网络的输出训练一个较小的网络，而不是使用Hinton最初提出的蒸馏法来防御对抗性手工制作。防御蒸馏最初是针对计算机视觉中的对抗性攻击进行测试的，但还需要进一步的研究来确定其在其他应用中的有效性，如恶意软件检测。

###### Adversarial Training（对抗训练）：Szegedy等人。[29]最初提出了一种称为对抗性训练的三步防御对抗性攻击的方法。1，在原始数据集上训练分类器2，生成对抗性样本3，使用对抗性样本迭代额外的训练阶段。对抗性训练提高了机器学习模型的分类性能，使其更能适应对抗性加工。

###### Detecting Adversarial Examples（检测对手示例）：在机器学习模型的训练阶段，有几种方法可用于检测对手示例的存在。文[76]提出的一种方法是在对抗性样本具有更高的不确定性的前提下工作的，这种不确定性是干净的数据，并使用贝叶斯神经网络来估计输入数据中的不确定性程度，以检测对抗性样本。其他方法包括使用由[79]提出的概率散度，以及使用由Metzen等人引入的原始网络的辅助网络。

###### Feature Reduction（特征缩减）：已经提出了对抗性攻击的其他潜在防御措施。Grosse等人对简单特征约简进行了评估。[77]但在对抗性攻击中被发现不足。

###### Ensemble Defenses（集成防御）：与集成学习的思想相类似，集成学习结合了一种或多种机器学习技术，研究人员还提出了使用多种防御策略作为对抗性例子的防御技术。PixelDefense是由[81]提出的，它将对抗性检测技术与一种或多种其他方法相结合，以创建更强大的对抗对抗性攻击的防御。

### 网络安全中的对抗攻击防御

介绍针对**网络安全**中的对抗性攻击提出的具体解决方案。

###### KUAFUDET伪装检测器：提高基于机器学习的恶意软件分类器准确性的一种方法是一种新型的恶意软件伪装检测器KUAFUDET，它可以显著地减少假阴性，并将检测精度提高至少15%。KUAFUDET采用两阶段学习增强方法，通过对抗性检测学习恶意软件样本的特征[82]。

###### SecureDroid：对抗基于机器学习的恶意软件检测的另一种方法是SecureDroid  defense[61]，它集成了SecCLS和SecENS两种方法来增强Android恶意软件检测。

###### SecDefender:SecDefender[32]提出了一种基于分类器再训练技术的恶意软件检测安全学习范式，以实现对规避攻击的恢复能力。

###### DroidEye：对抗性的android恶意软件攻击可以通过一个名为droidye的系统来防止，该系统实现了特征转换的计数特性，以增强机器学习分类器抵御攻击[31]。

###### SecureMD：在文[30]中提出的SecureMD中，通过使用安全正则化项，利用拟合约束和平滑约束，增强了机器学习恶意软件检测分类器。SecureMD将检测精度提高了93%。

###### Weighted Bagging（加权装袋）：Biggio等人。[83]提出了使用bagging分类器来防止基于机器学习的网络保护系统的中毒攻击。Bagging使用bootstrap聚合技术创建训练集的bootstrap副本。然后在bootstrap复制上训练分类器，并聚合预测。

###### 拒绝负面影响（RONI）：拒绝负面影响技术已被证明在检测入侵检测系统的对抗性攻击时达到100%的准确性。

###### Deepcloak:Ji  Gao等人介绍的深斗篷。[84]通过删除可能用于生成对抗性样本的不必要特征来工作。

### 特征空间与问题空间维度空间分类

在这一部分中，我们根据**特征空间**和问题空间对对抗性攻击进行分类。在机器学习领域，问题空间又称为**状态空间**，可以定义为问题确定上下文中对象的所有可能配置的维度表示。相反，特征空间被定义为n维空间，其中表示输入数据集中的所有变量。我们以一个包含70个变量的入侵检测数据集为例，它代表了一个70维的特征空间。

上述上下文中的特征空间对抗攻击将通过在70维特征空间内进行更改来寻求改变特征空间。**特征空间攻击直接修改实例中的特征，而问题空间对抗攻击修改实际实例本身**。以恶意软件对抗攻击为例，功能空间对抗恶意软件攻击只会**修改功能向量**，但不会创建新的恶意软件。问题空间敌对恶意软件攻击将从**源代码修改**实际实例以生成恶意软件的新实例。与问题空间对抗攻击相比，**特征空间对抗攻击不产生新样本，而是产生新的特征向量**。对抗样本的特征空间建模是一种利用优化算法从对特征所做的有限次任意改变中寻找理想值的方法。在特征空间对抗攻击中，攻击者的目标是保持良性而不生成新实例。各种对抗性攻击的特征空间和问题空间维度分类分别见表三和表四。从我们的观察来看，**问题空间中的对抗性攻击更难产生，也更难防御。**

网络安全中的**特征空间**对抗攻击及其有效性综述

![image-20200414205148126](C:\Users\LEO\AppData\Roaming\Typora\typora-user-images\image-20200414205148126.png)

网络安全中的**问题空间**对抗攻击及其有效性综述

![image-20200414205210370](C:\Users\LEO\AppData\Roaming\Typora\typora-user-images\image-20200414205210370.png)

### 总结与评价