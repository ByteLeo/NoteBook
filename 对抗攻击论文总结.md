# 对抗攻击论文总结

**一、对抗攻击简介**

对抗攻击是机器学习与计算机安全的结合（Intersection），是一个新兴的研究领域。以前设计的机器学习模型在面对攻击者精心设计的对抗攻击时往往会达不到预期的准确度，这种错误在如自动驾驶汽车等的实际应用中的影响是致命的。按照攻击者是否知道目的网络的结构参数，可以将对抗攻击分为**白盒攻击**和**黑盒攻击**。
对抗攻击采取的形式，就是攻击者根据某种算法给原始输入数据加入人为难以分辨的微小扰动从而构成对抗样本。目的网络将对抗样本作为输入时，往往会输出错误的分类结果。实际中，根据目的网络最终得到的分类结果是否是攻击者预先设计好的，将对抗攻击分为**目标攻击**和**非目标攻击**。

研究对抗攻击的意义如下：

**能让机器学习模型处理大规模数据；**

**以“计算机速度”处理攻击威胁；**

**不依赖数据的明显特征，发现实际应用中的各种内在威胁；**

**阻止已知和未知的恶意软件；**

**阻止恶意软件的提前执行；**

**优化模型，让分类模型达到更加高的分类准确率和更加低的错误率。**

我总结的对抗攻击方面的研究可以概括为下面几个方面：攻击原理探究、对抗攻击、对抗攻击防御、实际应用。

![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)



**二、攻击原理**

对产生对抗攻击的原理的探究，目前主要有以下几个方面的观点：

1.《Intriguing properties of neural networks》提出：深度神经网络模型的**非线性**导致的**输入与输出映射的不连续性**，加上**不充分的模型平均和不充分的正则化**导致的过拟合使得对抗攻击成为可能。

2.《Explaining And Harnessing Adversarial Examples》提出：高维空间中的**线性**就足以造成对抗样本，深度模型对对抗样本的脆弱性最主要的还是由于其**线性部分**的存在。通过将模型转变成非线性的RBF模型，就能减少神经网络模型对对抗攻击的脆弱性。

论文介绍了对抗样本存在的**线性解释**。

因为样本输入特征(input feature)的精度有限(一般图像的每个像素是8bits, 样本中所有低于1/255的信息都会被丢弃)，所以当样本 ![[公式]](https://www.zhihu.com/equation?tex=x) 中每个元素值添加的扰动值 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 小于样本输入特征精度时，分类器无法将样本 ![[公式]](https://www.zhihu.com/equation?tex=x) 和对抗样本 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D+%3D+x+%2B+%5Ceta) 区分开。也就是对一个分类良好的分类器而言，如果 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) 是一个足够小以至于被舍弃掉的值，那么只要 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7C%5Ceta%7C%7C_%7B%5Cinfty%7D+%3C+%5Cepsilon) ，分类器将认为 ![[公式]](https://www.zhihu.com/equation?tex=x) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D) 属于同一个类。下面考虑权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Comega%5E%7BT%7D) 和对抗样本 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D) 的点积为![[公式]](https://www.zhihu.com/equation?tex=%5Comega%5E%7BT%7D%5Ctilde%7Bx%7D+%3D%5Comega%5E%7BT%7D%28x+%2B%5Ceta%29+%3D+%5Comega%5E%7BT%7Dx+%2B+%5Comega%5E%7BT%7D%5Ceta)。可以看出，对抗扰动使得activation增加了 ![[公式]](https://www.zhihu.com/equation?tex=+%5Comega%5E%7BT%7D%5Ceta)，作者提出让 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+%3D+sign%28%5Comega%29) 从而使 ![[公式]](https://www.zhihu.com/equation?tex=+%5Comega%5E%7BT%7D%5Ceta) 最大化。假设权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Comega)有 ![[公式]](https://www.zhihu.com/equation?tex=n) 个维度，且权重向量中元素的平均量值是 ![[公式]](https://www.zhihu.com/equation?tex=m)，那么activation将增加![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+m+n+%28%5CRightarrow%5Comega%5E%7BT%7D%5Ceta+%5Cle+n%2Am%2A%5Cepsilon+%29)。虽然![[公式]](https://www.zhihu.com/equation?tex=%7C%7C%5Ceta%7C%7C_%7B%5Cinfty%7D+)不会随着维度![[公式]](https://www.zhihu.com/equation?tex=n)的变化而变化，但是由![[公式]](https://www.zhihu.com/equation?tex=%5Ceta)导致的activation的增加量![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+m+n)会随着维度![[公式]](https://www.zhihu.com/equation?tex=n)线性增长。那么对于一个高维度的问题，一个样本中大量维度的无限小的干扰加在一起就可以对输出造成很大的变化。

所以**对抗样本的线性解释**表明，对线性模型而言，如果其输入样本有足够大的维度，那么线性模型也容易受到对抗样本的攻击。

3. Karparthy博客 Breaking Linear Classifiers on ImageNet

[http://karpathy.github.io/2015](https://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/03/30/breaking-convnets/)

![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)



**三、Adversarial Attack**

目前构建对抗样本的方法很多，总结如下：

传统的梯度下降、牛顿法、BFGS、L-BFGS：这些方法在2013年发表的文章《Evasion attacks against machine learning at test time》和2014年发表的文章《Intriguing properties of neural networks》中提到并运用来生成对抗样本，同时，这两篇文章也是最早提出对抗攻击这个概念的。

**1、(1)  FGSM**：

《Explaining And Harnessing Adversarial Examples》（2015.3）

这篇Goodfellow提出的FGSM方法，是比较经典的对抗样本生成方法。（**通俗理解**：在图片梯度方向加上比较小的值） 

**(2)  Iterative version of FGSM**（The Basic Iterative Method ）：

Adversarial examples in the physical world》（2017.2）

这个方法可以视作是Fast Gradient Method的迭代应用。 这篇文章针对上一篇FGSM方法的扰动规模比较大的缺陷，提出构造规模更加受限的对抗样本。

**2、Jacobian saliency map attack (JSMA)** ：

《The limitations of deep learning in adversarial settings》（2015.10）

这篇文章提出的对抗样本构建方法，是建立在攻击者**已知目标网络的结构和参数等**信息的情况下。这种方法的核心思想，是通过**计算神经网络前向传播过程中的导数**生成对抗样本。

**3、One Pixel Attack：**

《One pixel attack for fooling deep neuralnetworks》（2017）

**4、DeepFool：**

《Deepfool: a simple and accurate method to fool deep neural networks》（2015.10）

这是第一个通过计算出最小的必要扰动，并应用到对抗样本构建的方法，使用的限制扰动规模的方法是L2范数。最终得到的对抗样本效果优于前面的FGSM和JSMA方法，但是这三者都需要比较大的计算资源。

**5、Papernot Method：**

《 Adversarial perturbations against deep neural networks for malware classification》（2016.6）

论文笔记：（[https://www.zybuluo.com/wuxin1994/note/854417](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/854417)）
这篇论文提到的对抗样本生成方法，更多的是针对于特定的应用场景——在输入是比较离散的数据情况下如何构建对抗样本。

**6、Universal Perturbations**： 

《Universal adversarial perturbations》（2016.10）

这是一种对DeepFool方法的延伸。
论文笔记：（[https://www.zybuluo.com/wuxin1994/note/847422](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/847422)）
《Analysis of universal adversarial perturbations》（2017.5）

**7、RP2：** 

《Robust Physical-World Attacks on Machine Learning Models》（2017.7）
论文笔记：（[https://www.zybuluo.com/wuxin1994/note/839621](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/839621)）

**8、CW（The Carlini and Wagner）**：

《Towards evaluating the robustness of neural networks》（2017.3）
这篇文章的作者将Szegedy在《Explaining And Harnessing Adversarial Examples》提出的攻击方式转化成了一个更加高效的优化问题，能够以添加更小扰动的代价得到更加高效的对抗样本。

**9、Virtual adversarial examples**：

《Virtual adversarial training: a regularization method for supervised and semi-supervised learning》（2017.4）

**10、**

《Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN》(2017.2)
论文笔记：（[https://www.zybuluo.com/wuxin1994/note/867495](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/867495)）

**11、**

《Machine Learning as an Adversarial Service: Learning Black-Box Adversarial Examples》（2017.8）
论文笔记：([https://www.zybuluo.com/wuxin1994/note/860472](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/860472))

![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)



**四、Defence Policy**

对抗攻击的防御策略总结如下：

**1、Adversarial Training**（augmenting the training data with perturbed examples）：

《Intriguing properties of neural networks》（2014）

所谓的对抗训练，就是防卫者通过自己构造对抗攻击，并且将人为增加扰动的对抗样本也加入到训练数据中，从而增强训练集，让训练后得到的模型更加稳定。

**（10.5新增）**《Towards deep learning models resistant to adversarial attacks 》提到对抗训练用比较弱的攻击时，往往并没有增加模型对更强的攻击的鲁棒性。（这篇文章对对抗训练的方法提出了**质疑**，即是这种方法是否真的可以应对未来将要遇到的更强的攻击？）

**2、PCA whitening** ：

《Early methods for detecting adversarial images》（2016.8）

**3、Defensive distillation：**

《Distillation as a defense to adversarial perturbations against deep neural networks》（2016.3）
这个方法通过两个步骤完成对模型稳定性的提升：第一步是训练分类模型，其最后一层的softmax层除以一个常数T；第二步是用同样的输入训练第二个模型，但是训练数据的标签不用原始标签，而是用第一步中训练的模型最后一层的概率向量作为最后softmax层的目标。
《Extending Defensive Distillation》（2017.5）

**4、Feature squeezing：**

《Feature squeezing: Detecting adversarial examples in deep neural networks》（2017.4）
《Feature squeezing mitigates and detects carlini/wagner adversarial examples》（2017.5）

**5、Detection systems**: 

（这种defence方法采取的策略是在目的网络模型前面增加一个额外的探测系统，判断输入是否是经过人为扰动的对抗样本）
Performe statistical tests:《On the (statistical) detection of adversarial examples》（2017.2）
Use an additional model for detection:《Adversarial and clean data are not twins》（2017.4）
《On detecting adversarial perturbations》（2017.2）
Apply dropout at test time:《Detecting adversarial samples from artifacts》（2017.3）

**6、Generative Adversarial Networks (GAN)**:

《Generative Adversarial Trainer Defense to Adversarial Perturbations with GAN》（2017.5）
《AE-GAN: adversarial eliminating with GAN》（2017.7）
论文笔记：（[https://www.zybuluo.com/wuxin1994/note/881171](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/881171)）

**7、**

《Efficient Defenses Against Adversarial Attacks》（2017.7）
论文笔记：([https://www.zybuluo.com/wuxin1994/note/863551](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/863551))

**8、**

《[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.00420)**》**
该论文中提到 发现了一种「混淆梯度」（obfuscated gradient）现象，它给对抗样本的防御带来虚假的安全感。在案例研究中，试验了 ICLR 2018 接收的 8 篇论文，发现混淆梯度是一种常见现象，其中有 7 篇论文依赖于混淆梯度，并被的这一新型攻击技术成功攻克。

![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)



**五、实际应用**

**Attacks on classification/recognition：**

1. 《Feature Space Perturbations Yield More Transferable Adversarial Examples》（cvpr2019）

更多UAP的论文：

1. UAP： 《Universal adversarial perturbations》（2016.10）：这是一种对DeepFool方法的延伸。
2. FFF：《Fast Feature Fool: A data independent approach to universal adversarial perturbations》（2017）：这是一个破坏卷积层特征的方法。
3. GDUAP：《Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations》：FFF的补充。（攻击分类、分割、深度估计三方面）
4. 结合生成模型：(1)《NAG: Network for Adversary Generation》论文笔记：[隅子酱：读论文|NAG: Network for Adversary Generation](https://zhuanlan.zhihu.com/p/58374906) (2)《Learning Universal Adversarial Perturbations with Generative Models》
5. AAA：《Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions》引入Class Impression，论文笔记：[隅子酱：读论文 | Ask, Acquire, and Attack: Data-free UAP](https://zhuanlan.zhihu.com/p/63291662)
6. singular vectors：《Art of singular vectors and universal adversarial perturbations》

**Attacks on Semantic Segmentation and Object Detection：**

1. 《Adversarial examples for semantic segmentation and object detection》In ICCV, 2017.
2. 《Adversarial examples that fool detectors》
3. 《Robust adversarial perturbation on deep proposal-based models》 In BMVC, 2018.
4. 《Attacking object detectors via imperceptible patches on background.》
5. GDUAP：《Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations》：FFF的补充。（攻击分类、分割、深度估计三方面）
6. 《Transferable Adversarial Attacks for Image and Video Object Detection》（2019）笔记：[隅子酱：读论文 | Feature Space Perturbations...特征空间可迁移对抗样本](https://zhuanlan.zhihu.com/p/83171428)

**Attacks in the real world：**

1. 面部识别：《Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition》（2016.10）

2. 实际拍照图片：《Adversarial examples in the physical world》（2017.2）
   这篇文章是在实际应用中，对抗攻击往往不能将数字化的对抗样本作为目的分类器的输入，只能将对抗样本打印到纸张上，然后用拍照之类的方式得到目的网络的输入时，人为添加的扰动比较小，在拍照过程中产生了失真，不能达到攻击目的。

3. 路标：(1)《Robust Physical-World Attacks on Machine Learning Models》（2017.7）
   论文笔记：[隅子酱：Robust Physical-World Attacks](https://zhuanlan.zhihu.com/p/44531740) (2)《Note on Attacking Object Detectors with Adversarial Stickers》

4. 自动汽车：(1)《Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning》（2017）
   论文笔记：([https://www.zybuluo.com/wuxin1994/note/843327](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/843327)) (2)《No need to worry about adversarial examples in object detection in autonomous vehicles》

5. 恶意软件分类：《Adversarial Perturbations Against Deep Neural Networks for Malware Classification》（2016.6）
   论文笔记：([https://www.zybuluo.com/wuxin1994/note/854417](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/854417))
   《Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN》（2017.5）
   论文笔记：（[https://www.zybuluo.com/wuxin1994/note/867495](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/867495)）

6. 3D打印：《Synthesizing Robust Adversarial Examples》

   ![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)

   

**六、其他研究**

对抗攻击可移植性研究：

1. 《Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples》（2016.5） 论文笔记：([https://www.zybuluo.com/wuxin1994/note/850755](https://link.zhihu.com/?target=https%3A//www.zybuluo.com/wuxin1994/note/850755))
2. 《[Delving into Transferable Adversarial Examples and Black-box Attacks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.02770)》

![img](https://pic2.zhimg.com/80/v2-1e4bb5487dc39e219deda7c0ce73cd49_720w.png)

**七、总结**
根据目前学习的对抗攻击研究，我觉得以后的**研究方向**主要有以下几个方面：

1. 效率更高的对抗样本构造方法；

2. 更好的Defence策略构建；

3. 根据特定的应用场景探究攻击和防御策略（垃圾邮件分类、恶意软件识别、人脸识别等与安全相关的领域）；
4. 得到对抗样本，在模型的训练集中加入对抗攻击样本，可以增强神经网络的鲁棒性；
5.  理解对抗攻击背后的数学原理，实际就是探索深度网络的原理，尝试打开这个黑盒子。
6. （9.18更新）*已经知道了根据对目的模型的了解程度可以造成不同的影响结果，那么能否根据这一点来探究各个影响因素分别的效果呢？*
7. *（10.4更新）*存在这样一个问题：很难去评判一个defence方法是否是足够有效的，也很难去评价一个攻击方法是否是足够成功的。因为往往在对抗攻击研究进程中，一个提出的defence策略总是会被后来提出的攻击方法证明是不够鲁棒的。反之，一个攻击方法也往往会被后面提出的defence方法证明是无效的。这种往复的循环博弈，给研究指出了一个新的方向:可以研究一种评估攻击方法或者防御策略的有效性的评估方法。
   这一部分参考《Ground-Truth Adversarial Examples》：a defensive technique that was at first thought to produce robust networks was later shown to be susceptible to new kinds of attacks.

**将GAN与对抗攻击研究相结合的方向的论文不日更新。**它可以同时构建效果更加好的对抗样本和实现让模型更加鲁棒的defence策略。对抗攻击可以实现，其本质一方面是因为神经网络乃至深度学习可以实现分类和预测目的的原理还比较模糊，因此可以利用这种不确定性来混淆模型；另一方面是因为数据本身就不能按照抽取的特征得到固定的分类结果，每一个个体具有比较大的误差因素。因此，模型容易受到对抗攻击是因为模型的泛化能力不够，在处理非训练数据时容易得到错误的结果。提高模型的泛化能力才是最好的defence策略。而最近特别火的GAN网络正是一种提高网络泛化能力的手段，在多个方面都被证明能让训练得到的模型具有更好的效果。

![preview](https://pic2.zhimg.com/v2-1e4bb5487dc39e219deda7c0ce73cd49_r.jpg)



**关于怎么找研究方向相关论文：**

1. Google](https://link.zhihu.com/?target=http%3A//1.Google) /度娘搜索关键字，拿到一些论文。
2. 看论文的参考文献。（比较老但是经典的论文）
3. 看论文被引的论文。（新）

- 两个方法可以查看：

(1)谷歌学术(论文标题下面有写引用)

(2)翻不了墙就用[https://www.researchgate.net/institution/ResearchGate](https://link.zhihu.com/?target=https%3A//www.researchgate.net/institution/ResearchGate)